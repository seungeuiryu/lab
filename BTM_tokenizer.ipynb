{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BTM_tokenizer.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXP00JzIoI4G7F3PLrJCsY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiu0327/lab/blob/main/BTM_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVb3FkLUWHgH"
      },
      "source": [
        "import re\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRpHhINEWPa0"
      },
      "source": [
        "file = pd.read_csv('/content/data2.csv', encoding='utf-8')\n",
        "#file = pd.read_excel('/content/The Maarble Mountains.xlsx')\n",
        "reviews = file['contents']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0B3HViOZ8fr"
      },
      "source": [
        "print(reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F34rH5JPaYk9"
      },
      "source": [
        "delWord = []\n",
        "\n",
        "with open('/content/Lady buddha exception.txt', encoding='utf-8') as f:\n",
        "    for word in f:\n",
        "        delWord.append(word.replace('\\n', ''))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOE5w58aau-c"
      },
      "source": [
        "print(delWord)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZNFuqDIgHQE"
      },
      "source": [
        "#appendDel = ['Danang', 'danang', 'vietnam']\n",
        "#delWord = delWord+appendDel\n",
        "#print(delWord)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIpy1Tn0bcPg"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gA2BneCbmQ-"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atzQemTabYqE"
      },
      "source": [
        "sents = [sent_tokenize(rv) for rv in reviews]\n",
        "sents = sum(sents, [])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM_p9A1kbom7"
      },
      "source": [
        "print(sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OfThmPEcCy1"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGK46XOjcGSE"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWCP-y6Dca24"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU1rcmWZ77vy"
      },
      "source": [
        "print(sents[0])\n",
        "print(\"Da Nang is good.\".replace('Da Nang', ''))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTgPTHdcZ-rY"
      },
      "source": [
        "import csv\n",
        "\n",
        "def tokenize(text):\n",
        "\n",
        "    for d in delWord:\n",
        "        text.replace(d, '')\n",
        "\n",
        "    text = re.sub('[^a-zA-Z0-9 ]', '', text).strip()\n",
        "\n",
        "    # tokenize\n",
        "    wds = [wd for wd in word_tokenize(text)\n",
        "           if wd not in stopwords.words('english')]\n",
        "\n",
        "    for w in wds:\n",
        "        if w in delWord:\n",
        "            wds.remove(w)\n",
        "\n",
        "    print(wds)\n",
        "    \n",
        "    \n",
        "    # tag pos\n",
        "    pos = ['JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS',\n",
        "           'RP', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "    wds_pos = [wd for wd in pos_tag(wds) if wd[1] in pos]\n",
        "\n",
        "    # lemmatize\n",
        "    wds_pos_lem = []\n",
        "    for wd in wds_pos:\n",
        "        try:\n",
        "            wd = lemmatizer.lemmatize(wd[0], wd[1][0].lower())\n",
        "        except:\n",
        "            wd = wd[0]\n",
        "        wds_pos_lem.append(wd)\n",
        "\n",
        "    return wds_pos_lem\n",
        "\n",
        "\n",
        "rv_processed = [tokenize(rv) for rv in reviews]\n",
        "file = open('review_token.csv', 'w', encoding='utf-8', newline='')\n",
        "writer = csv.writer(file)\n",
        "writer.writerows([rv for rv in rv_processed if rv])\n",
        "\n",
        "st_processed = [tokenize(sent) for sent in sents]\n",
        "#file = open('sentence_token.csv', 'w', encoding='utf-8', newline='')\n",
        "#writer = csv.writer(file)\n",
        "#writer.writerows([st for st in st_processed if st])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2K0IuG3coRA"
      },
      "source": [
        "st_processed = [tokenize(sent) for sent in sents]\n",
        "file = open('sentence_token.csv', 'w', encoding='utf-8', newline='')\n",
        "writer = csv.writer(file)\n",
        "writer.writerows([st for st in st_processed if st])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb3uBPwxbOk9"
      },
      "source": [
        "df = pd.DataFrame(st_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4vUF9btbPug"
      },
      "source": [
        "df.to_csv('sentence_token_Lady.csv', sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epiYu4p8jF-M"
      },
      "source": [
        "outFile = open('sentence_token_Lady.txt', 'w', encoding='utf-8')\n",
        "for a1 in st_processed:\n",
        "    tmp = \"\"\n",
        "    for a2 in a1:\n",
        "        tmp+=a2+\",\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wJj3CFpZu7v"
      },
      "source": [
        "print(sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1jx_QsSZxxC"
      },
      "source": [
        "st_processed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSI58leyaRx3"
      },
      "source": [
        "with open('sentence_token_lady_test.csv', 'w', newline='') as f:\n",
        "    write = csv.writer(f)\n",
        "    write.writerows([st for st in st_processed if st])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}