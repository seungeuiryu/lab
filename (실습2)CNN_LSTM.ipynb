{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "자연어처리 코드 흐름 정리.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiu0327/lab/blob/main/(%EC%8B%A4%EC%8A%B51)CNN_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) 데이터 셋 불러오기"
      ],
      "metadata": {
        "id": "KWGWdAn4qGFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "id": "9YGGMWdtrd0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_data=pd.read_csv('/content/train_data.csv', encoding='utf-8')\n",
        "test_data = pd.read_csv('/content/test_data.csv', encoding='utf-8')"
      ],
      "metadata": {
        "id": "zcZih8iGsZlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "8ZaY4wo-y8OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 데이터 중복 체크\n",
        "train_data['text'].nunique(), train_data['label'].nunique()\n",
        "\n",
        "# 데이터 중복 제거\n",
        "train_data.drop_duplicates(subset='text', inplace=True)\n",
        "test_data.drop_duplicates(subset='text', inplace=True)\n",
        "\n",
        "# null값 유무\n",
        "print(train_data.isnull().values.any())\n",
        "print(test_data.isnull().values.any())\n",
        "\n",
        "# 데이터 전처리\n",
        "train_data['text'] = train_data['text'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "train_data['text'].replace('', np.nan, inplace=True)\n",
        "print(train_data.isnull().sum())\n",
        "\n",
        "train_data = train_data.dropna(how='any') # null 값 제거\n",
        "\n",
        "print(train_data.isnull().sum())\n",
        "\n",
        "# 불용어 제거\n",
        "from tqdm import tqdm\n",
        "okt = Okt()\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
        "X_train = []\n",
        "for sentence in tqdm(train_data['text']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    X_train.append(stopwords_removed_sentence)\n"
      ],
      "metadata": {
        "id": "7nf2eA1DqJ_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 정수 인코딩\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# 단어 집합이 생기는 동시에 각 단어에 고유한 정수가 부여됨. 등장횟수가 1회인 단어들은 배제.\n",
        "threshold = 2\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
        "\n",
        "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
        "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
        "vocab_size = total_cnt - rare_cnt + 2\n",
        "print('단어 집합의 크기 :',vocab_size)"
      ],
      "metadata": {
        "id": "-xL3oJXvsYTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) 단어 임베딩"
      ],
      "metadata": {
        "id": "L-6BROizw77-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(vocab_size, oov_token='OOV')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)"
      ],
      "metadata": {
        "id": "tshAfME6sgwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩 작업\n",
        "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))"
      ],
      "metadata": {
        "id": "brmPfNZrw92P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "    count = 0\n",
        "    for sentence in nested_list:\n",
        "        if(len(sentence) <= max_len):\n",
        "            count = count+1\n",
        "    print(\"전체 샘플 중 길이가 \"+str(max_len)+\" 이하인 샘플의 비율 : {}\".format((count / len(nested_list))*100))\n",
        "\n",
        "max_len = 100\n",
        "below_threshold_len(max_len, X_train)"
      ],
      "metadata": {
        "id": "fhAO0o5yw-by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) 패딩"
      ],
      "metadata": {
        "id": "1rIMKX460JgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(X_train, maxlen=max_len)"
      ],
      "metadata": {
        "id": "YJZzQpaK0Kdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) 모델 설정"
      ],
      "metadata": {
        "id": "fAIDjR6V0C1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint  \n",
        "\n",
        "model = Sequential()\n",
        "embedding_dim = 100\n",
        "hidden_units= 128\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(1,activation='sigmoid'))"
      ],
      "metadata": {
        "id": "N6OFzSKexBTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "embedding_dim = 256\n",
        "dropout_ratio = 0.3\n",
        "num_filters = 256\n",
        "kernel_size = 3\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(Dropout(dropout_ratio))\n",
        "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(hidden_units, activation='relu'))\n",
        "model.add(Dropout(dropout_ratio))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "7Y2f7DocsNhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "embedding_dim = 256\n",
        "dropout_ratio = 0.3\n",
        "num_filters = 256\n",
        "kernel_size = 3\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(Dropout(dropout_ratio))\n",
        "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dropout(dropout_ratio))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "aPiQaY0ixNS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "y_train = train_data['label'].values"
      ],
      "metadata": {
        "id": "FZEWEZr3z_3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) 모델 훈련"
      ],
      "metadata": {
        "id": "lXqD4KuT1hEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "history=model.fit(X_train, y_train, epochs=1, callbacks=[es,mc], batch_size=256, validation_split=0.2)"
      ],
      "metadata": {
        "id": "YWvwHlCr2ExH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv('/content/test_data.csv', encoding='utf-8')\n",
        "print(\"전체 테스트 데이터 개수 : {}\".format(len(test_data)))\n",
        "\n",
        "test_data['text'].nunique()\n",
        "\n",
        "test_data.drop_duplicates(subset=['text'], inplace=True)\n",
        "print(\"중복을 제거한 테스트 데이터 개수 : {}\".format(len(test_data)))\n",
        "\n",
        "print(test_data.isnull().values.any())\n",
        "\n",
        "test_data['text'] = test_data['text'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "test_data['text'].replace('', np.nan, inplace=True)\n",
        "test_data = test_data.dropna(how='any')\n",
        "print(\"전처리 후 테스트 데이터 개수 : {}\".format(len(test_data)))\n",
        "\n",
        "X_test = []\n",
        "for sentence in tqdm(test_data['text']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True)\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords]\n",
        "    X_test.append(stopwords_removed_sentence)\n",
        "\n",
        "# test 데이터 정수 인코딩\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "print(X_test)\n",
        "\n",
        "X_test = pad_sequences(X_test, maxlen = max_len)\n",
        "y_test = test_data['label'].values\n",
        "\n"
      ],
      "metadata": {
        "id": "pzJVqRj60Mtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "id": "GKdENm2E2N7_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
