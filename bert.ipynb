{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFQwFkl32Q4Ve3588y5x8K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiu0327/lab/blob/main/bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTvavS9S8CNe"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0h3TuZB8qwW"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torchtext.legacy import data\n",
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c76ZkPK8vx7"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "#랜덤시드 고정\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "#bert 라이브러리에서 제공하는 토크나이저 사용\n",
        "\n",
        "tokens = tokenizer.tokenize('우리 사이엔 낮은 담이 있어 서로의 진심을 안을 수가 없어요')\n",
        "print(tokens)\n",
        "\n",
        "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(indexes)\n",
        "\n",
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)\n",
        "\n",
        "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
        "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
        "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
        "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)\n",
        "\n",
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-multilingual-cased']\n",
        "print(max_input_length)\n",
        "\n",
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens\n",
        "\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                 use_vocab = False,\n",
        "                 tokenize = tokenize_and_cut,\n",
        "                 preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                 init_token = init_token_idx,\n",
        "                 eos_token = eos_token_idx,\n",
        "                 pad_token = pad_token_idx,\n",
        "                 unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "\n",
        "fields = {'text': ('text',TEXT), 'label': ('label',LABEL)}\n",
        "\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "    path='/content/',\n",
        "    train='train_data.csv',\n",
        "    test='test_data.csv',\n",
        "    format='csv',\n",
        "    fields=fields,\n",
        ")\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED))\n",
        "\n",
        "print(f'훈련 데이터 갯수: {len(train_data)}')\n",
        "print(f'검증 데이터 갯수: {len(valid_data)}')\n",
        "print(f'테스트 데이터 갯수: {len(test_data)}')\n",
        "\n",
        "print(vars(train_data.examples[10]))\n",
        "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[10])['text'])\n",
        "print(tokens)\n",
        "\n",
        "string = tokenizer.convert_tokens_to_string(tokens)\n",
        "print(string)\n",
        "\n",
        "LABEL.build_vocab(train_data)\n",
        "#라벨 단어장\n",
        "\n",
        "print(LABEL.vocab.stoi)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch = True,\n",
        "    device = device)\n",
        "\n",
        "#모델 생성\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "# 사전 훈련된 모델 - bert는 위키피디아 같은 자료로 미리 학습한 모델을 제공한다.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuvXKIir_qLl"
      },
      "source": [
        "class BERTGRUSentiment(nn.Module):\n",
        "    def __init__(self, bert, hidden_dim, output_dim,\n",
        "                 n_layers, bidirectional, dropout):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim,\n",
        "                          num_layers=n_layers,\n",
        "                          bidirectional=bidirectional,\n",
        "                          batch_first=True,\n",
        "                          dropout=0 if n_layers < 2 else dropout)\n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional\n",
        "                             else hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text = [batch_size, sent_len]\n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(text)[0]\n",
        "            # embedded = [batch_size, sent_len, emb_dim]\n",
        "\n",
        "        _, hidden = self.rnn(embedded)\n",
        "        # hideen = [n_layers * n_directions, batch_size, emb_dim]\n",
        "\n",
        "        if self.rnn.bidirectional:\n",
        "            # 마지막 레이어의 양방향 히든 벡터만 가져옴\n",
        "            hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1, :, :])\n",
        "        # hidden = [batch_size, hid_dim]\n",
        "\n",
        "        output = self.out(hidden)\n",
        "        # output = [batch_size, out_dim]\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.25\n",
        "\n",
        "model = BERTGRUSentiment(bert, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                        N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'모델의 파라미터 수는 {count_parameters(model):,}, 이 중 버트 모델의 파라미터 수는 {count_parameters(bert):,}개입니다.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po-nYzgj_wj5"
      },
      "source": [
        "for name, param in model.named_parameters():\n",
        "\n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False\n",
        "\n",
        "# 버트 모델의 파라미터는 훈련시키면 안됨. 따라서 모델의 파라미터는 동결\n",
        "\n",
        "print(f'모델의 파라미터 수는 {count_parameters(model):,}개입니다.')\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "\n",
        "    if param.requires_grad == True:\n",
        "        print(name)\n",
        "\n",
        "# 모델 구조 확인\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJOHcO48_0a4"
      },
      "source": [
        "print('aaa')\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds==y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch.text).squeeze(1)  # output_dim = 1\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnHpdzai_34W"
      },
      "source": [
        "#훈련시작\n",
        "\n",
        "N_EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in tqdm(range(N_EPOCHS)):\n",
        "    print('aaa')\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DmFrtpNJrpp"
      },
      "source": [
        "#모델 성능 확인\n",
        "\n",
        "model.load_state_dict(torch.load('tut6-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAgq4Qi5LNmY"
      },
      "source": [
        "# 추가훈련\n",
        "\n",
        "N_EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in tqdm(range(N_EPOCHS)):\n",
        "    print('aaa')\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch + 6:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7Qxrq5AVaIp"
      },
      "source": [
        "def predict_sentiment(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    return prediction.item()\n",
        "\n",
        "\n",
        "review_data = pd.read_csv('/content/38766_data.csv', encoding='utf-8')\n",
        "sentence = review_data['content']\n",
        "\n",
        "output = open('기계학습_감정분석 결과3 -bert.txt', 'w', encoding='utf-8')\n",
        "\n",
        "print('결과값 도출 시작')\n",
        "\n",
        "#with open('/content/38766_output.txt', 'r', encoding='utf-8') as f:\n",
        "    #for sen in f:\n",
        "        #output.write(\"평가 문장 : \"+sen+'\\n')\n",
        "        #output.write(\"결과 : \"+str(predict_sentiment(model, tokenizer, sen))+'\\n')\n",
        "\n",
        "for sen in sentence:\n",
        "    output.write(\"평가 문장 : \"+sen+'\\n')\n",
        "    output.write(\"결과 : \"+str(predict_sentiment(model, tokenizer, sen))+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}